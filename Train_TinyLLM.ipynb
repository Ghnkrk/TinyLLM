{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece datasets\n",
        "!pip install wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgQseyOIFEIG",
        "outputId": "f2ca1d27-f242-4ca2-c0c9-aebfe338fb12"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.46.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "zoEi7-T4dYWC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from typing import Optional\n",
        "import sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from random import shuffle\n",
        "import math\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyVQIBgYUP55",
        "outputId": "60d36d7f-c05b-4c03-f840-1aa2073b63c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mguhan-karthik25\u001b[0m (\u001b[33mguhan-karthik25-kumaraguru-college-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_causal_mask(seq_len: int, device: Optional[torch.device] = None):\n",
        "    \"\"\"\n",
        "    Additive mask (T, T) with 0 for allowed, -1e9 for blocked future positions.\n",
        "    Suitable to add to attention logits.\n",
        "    \"\"\"\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
        "    mask = mask.float().masked_fill(mask, float(\"-1e9\"))\n",
        "    return mask.to(device) if device is not None else mask"
      ],
      "metadata": {
        "id": "fqadpkIFDu-l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IKRcqlmweIqc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, max_len=2048):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(input_dim, 3 * d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # For cross-attn\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # RoPE module\n",
        "        self.rope = RotaryEmbedding(self.head_dim, max_len=max_len)\n",
        "\n",
        "    def _apply_rope(self, q, k):\n",
        "        # q, k: (B, H, T, Dh)\n",
        "        return self.rope(q), self.rope(k)\n",
        "\n",
        "    def _attention(self, q, k, v, mask=None):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "            scores = scores + mask.to(scores.device)\n",
        "\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        return torch.matmul(weights, v)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, _ = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)  # (B, T, 3D)\n",
        "        qkv = qkv.view(B, T, self.num_heads, 3*self.head_dim).permute(0,2,1,3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Apply RoPE\n",
        "        q, k = self._apply_rope(q, k)\n",
        "\n",
        "        out = self._attention(q, k, v, mask)\n",
        "        out = out.permute(0,2,1,3).contiguous().view(B, T, -1)\n",
        "        return self.out_proj(out)\n",
        "\n",
        "    def forward_cross_attention(self, q_in, k_in, v_in, mask=None):\n",
        "        B, Tq, _ = q_in.shape\n",
        "        _, Tk, _ = k_in.shape\n",
        "\n",
        "        q = self.q_linear(q_in).view(B, Tq, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
        "        k = self.k_linear(k_in).view(B, Tk, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
        "        v = self.v_linear(v_in).view(B, Tk, self.num_heads, self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        # Apply RoPE to cross-attn q,k\n",
        "        q, k = self._apply_rope(q, k)\n",
        "\n",
        "        out = self._attention(q, k, v, mask)\n",
        "        out = out.permute(0,2,1,3).contiguous().view(B, Tq, -1)\n",
        "        return self.out_proj(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiDpkfwD4LMC"
      },
      "source": [
        "Positional Encoding\n",
        "Rotary q,k embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "94AP8Jvy4gDe"
      },
      "outputs": [],
      "source": [
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, head_dim, max_len=2048):\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "        # Compute base frequencies\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
        "\n",
        "        # Precompute sin/cos tables\n",
        "        t = torch.arange(max_len)\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
        "\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)  # interleave\n",
        "        self.register_buffer(\"cos_emb\", emb.cos())\n",
        "        self.register_buffer(\"sin_emb\", emb.sin())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, H, T, D)\n",
        "        apply RoPE on last dimension D\n",
        "        \"\"\"\n",
        "        B, H, T, D = x.shape\n",
        "        cos = self.cos_emb[:T].unsqueeze(0).unsqueeze(0)\n",
        "        sin = self.sin_emb[:T].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        x1 = x[..., : D//2]\n",
        "        x2 = x[..., D//2 :]\n",
        "\n",
        "        # rotate\n",
        "        x_rot = torch.cat([-x2, x1], dim=-1)\n",
        "        return (x * cos) + (x_rot * sin)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=2048):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x   # RoPE replaces absolute positional embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzxsDRxneYMC"
      },
      "source": [
        "**Feed Forward Neural Network**\n",
        "```\n",
        "transform the data to higher dimension to capture complex information.\n",
        "relu activation to introduce non-linearity.\n",
        "transform back to lower dimension of d_model.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JB8uQmQVXMbI"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ffn: int, activation=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ffn),\n",
        "            activation,\n",
        "            nn.Linear(d_ffn, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9X57ICKfksG"
      },
      "source": [
        "**Encoding layer**\n",
        "```\n",
        "structuring the\n",
        "1.positional encoding\n",
        "2.multihead attention\n",
        "3.add and normalization\n",
        "4.feed forward neural network\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NDReUwc1e8k9"
      },
      "outputs": [],
      "source": [
        "class EncodingLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ffn: int):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.mha = MultiHeadAttention(d_model, d_model, num_heads)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForwardNetwork(d_model, d_ffn)\n",
        "\n",
        "    def forward(self, x, src_mask: Optional[torch.Tensor] = None):\n",
        "        # Pre-norm: LN -> MHA -> resid\n",
        "        x = x + self.mha(self.ln1(x), mask=src_mask)\n",
        "        # FFN block\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHdUit4zYd9_"
      },
      "source": [
        "**Transformer Encoder**\n",
        "\n",
        "\n",
        "one or more encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AL2WB7hlL1mL"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, d_ffn: int, max_len: int = 2048):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
        "        self.layers = nn.ModuleList([EncodingLayer(d_model, num_heads, d_ffn) for _ in range(num_layers)])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, src_mask: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        input_ids: (B, T)\n",
        "        returns (B, T, d_model)\n",
        "        \"\"\"\n",
        "        x = self.token_emb(input_ids)  # (B, T, d_model)\n",
        "        x = self.pos_enc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return self.ln(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYEW35mmbsEr"
      },
      "source": [
        "**Decoder layer**\n",
        "```\n",
        "structuring:\n",
        "1. Positional encoder for decoder input.\n",
        "2. multihead attention with mask\n",
        "3. layer normalization.\n",
        "4. multihead attention without mask with encoder output and decoder data as input\n",
        "5. layer normalization\n",
        "6. feed forward network\n",
        "7. layer normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fr7xGIhabIwg"
      },
      "outputs": [],
      "source": [
        "class DecodingLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, d_ffn: int):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.self_att = MultiHeadAttention(d_model, d_model, num_heads)\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.cross_att = MultiHeadAttention(d_model, d_model, num_heads)\n",
        "\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForwardNetwork(d_model, d_ffn)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, enc_output: Optional[torch.Tensor] = None, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None):\n",
        "        # masked self-attention (pre-norm)\n",
        "        x = x + self.self_att(self.ln1(x), mask=tgt_mask)\n",
        "\n",
        "        # cross-attention only if encoder output is provided\n",
        "        if enc_output is not None:\n",
        "            x = x + self.cross_att.forward_cross_attention(self.ln2(x), enc_output, enc_output, mask=src_mask)\n",
        "\n",
        "        # feed-forward\n",
        "        x = x + self.ff(self.ln3(x))\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCjKud8tyHFn"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y4PGNQHqx7go"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, d_ffn: int, max_len: int = 2048):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
        "        self.layers = nn.ModuleList([DecodingLayer(d_model, num_heads, d_ffn) for _ in range(num_layers)])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, enc_output: Optional[torch.Tensor] = None, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None):\n",
        "        B, T = input_ids.shape\n",
        "        x = self.token_emb(input_ids)\n",
        "        x = self.pos_enc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_output=enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "        return self.ln(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder only wrapper\n"
      ],
      "metadata": {
        "id": "L_hJ-h4WEYDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, d_ffn: int, max_len: int = 2048):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size, d_model, num_layers, num_heads, d_ffn, max_len=max_len)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor, src_mask: Optional[torch.Tensor] = None):\n",
        "        enc = self.encoder(input_ids, src_mask=src_mask)\n",
        "        enc = self.ln(enc)\n",
        "        logits = self.head(enc)  # (B, T, V)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "C6JYtd9uEPqF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder only wrapper"
      ],
      "metadata": {
        "id": "bJdwWYZ1EbzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, d_ffn: int, max_len: int = 2048):\n",
        "        super().__init__()\n",
        "        # build a decoder; token embedding sized to vocab\n",
        "        self.decoder = Decoder(vocab_size, d_model, num_layers, num_heads, d_ffn, max_len=max_len)\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        input_ids: (B, T)\n",
        "        returns logits: (B, T, V)\n",
        "        \"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        tgt_mask = generate_causal_mask(T, device=input_ids.device)\n",
        "        dec = self.decoder(input_ids, enc_output=None, src_mask=None, tgt_mask=tgt_mask)\n",
        "        dec = self.ln(dec)\n",
        "        logits = self.head(dec)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "TOghBRzOESOs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_xavier(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "        if getattr(module, \"weight\", None) is not None:\n",
        "            nn.init.ones_(module.weight)\n",
        "        if getattr(module, \"bias\", None) is not None:\n",
        "            nn.init.zeros_(module.bias)"
      ],
      "metadata": {
        "id": "EOrr8diCEVQt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 2000\n",
        "d_model = 128\n",
        "num_layers = 4\n",
        "num_heads = 4\n",
        "d_ffn = 512\n",
        "max_len = 256"
      ],
      "metadata": {
        "id": "GH5dM5nBElAM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecoderOnlyTransformer(vocab_size, d_model, num_layers, num_heads, d_ffn, max_len=max_len)\n",
        "model.apply(init_weights_xavier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kX2H1WSEp_U",
        "outputId": "a685df3e-4941-466e-8aff-6d9f091766f9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderOnlyTransformer(\n",
              "  (decoder): Decoder(\n",
              "    (token_emb): Embedding(2000, 128)\n",
              "    (pos_enc): PositionalEncoding()\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x DecodingLayer(\n",
              "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_att): MultiHeadAttention(\n",
              "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (rope): RotaryEmbedding()\n",
              "        )\n",
              "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (cross_att): MultiHeadAttention(\n",
              "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (rope): RotaryEmbedding()\n",
              "        )\n",
              "        (ln3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForwardNetwork(\n",
              "          (net): Sequential(\n",
              "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=128, out_features=2000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyI3-p0TEvR0",
        "outputId": "20064250-02af-4615-bc34-df408676450b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderOnlyTransformer(\n",
              "  (decoder): Decoder(\n",
              "    (token_emb): Embedding(2000, 128)\n",
              "    (pos_enc): PositionalEncoding()\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x DecodingLayer(\n",
              "        (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_att): MultiHeadAttention(\n",
              "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (rope): RotaryEmbedding()\n",
              "        )\n",
              "        (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (cross_att): MultiHeadAttention(\n",
              "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (q_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (k_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (v_linear): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (rope): RotaryEmbedding()\n",
              "        )\n",
              "        (ln3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForwardNetwork(\n",
              "          (net): Sequential(\n",
              "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (1): ReLU()\n",
              "            (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=128, out_features=2000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsIB3JwHFdW9",
        "outputId": "5d73cee3-e8fc-4909-fa4d-82bb906ea815"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 2119719\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 21990\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_path = \"tinystories_corpus.txt\"\n",
        "\n",
        "with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in dataset[\"train\"]:\n",
        "        story = item[\"text\"].replace(\"\\n\", \" \")\n",
        "        f.write(story.strip() + \"\\n\")\n"
      ],
      "metadata": {
        "id": "S6Un3kjxF6q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 2000\n",
        "model_prefix = \"tinystories_sp\"\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=corpus_path,\n",
        "    model_prefix=model_prefix,\n",
        "    vocab_size=vocab_size,\n",
        "    character_coverage=1.0,\n",
        "    model_type=\"bpe\",\n",
        "    pad_id=0,\n",
        "    unk_id=1,\n",
        "    bos_id=2,\n",
        "    eos_id=3\n",
        ")"
      ],
      "metadata": {
        "id": "iTqQLXZ8F9JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(f\"{model_prefix}.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd4cdVjCGFtE",
        "outputId": "f476bcf4-a1c0-42af-b703-d03fce140ec9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyStoriesDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, tokenizer, block_size=128):\n",
        "        self.data = hf_dataset\n",
        "        self.sp = tokenizer\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx][\"text\"]\n",
        "        ids = self.sp.encode(text, out_type=int)\n",
        "\n",
        "        # Always ensure at least block_size+1 tokens\n",
        "        if len(ids) < self.block_size + 1:\n",
        "            # pad the sequence\n",
        "            needed = (self.block_size + 1) - len(ids)\n",
        "            ids = ids + [0] * needed\n",
        "\n",
        "        # SAFE: compute max starting index\n",
        "        max_start = len(ids) - (self.block_size + 1)\n",
        "\n",
        "        # If max_start == 0 → only one valid window\n",
        "        if max_start == 0:\n",
        "            start = 0\n",
        "        else:\n",
        "            start = torch.randint(0, max_start, (1,)).item()\n",
        "\n",
        "        chunk = ids[start : start + self.block_size + 1]\n",
        "\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "myUle1jxGhrO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp.encode(\"Hello! I am training a tiny transformer.\", out_type=int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBGMTeZPGJTd",
        "outputId": "b91db3fd-1c29-4208-e12d-1965bd4355bd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[62,\n",
              " 1692,\n",
              " 1891,\n",
              " 81,\n",
              " 885,\n",
              " 1438,\n",
              " 37,\n",
              " 6,\n",
              " 1723,\n",
              " 454,\n",
              " 47,\n",
              " 1867,\n",
              " 1878,\n",
              " 49,\n",
              " 1064,\n",
              " 1873]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = TinyStoriesDataset(dataset[\"train\"], sp, block_size=128)\n",
        "val_ds = TinyStoriesDataset(dataset[\"validation\"], sp, block_size=128)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32)\n",
        "\n"
      ],
      "metadata": {
        "id": "QQprTq64GtAe"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp, tgt = next(iter(train_loader))\n",
        "inp = inp.to(device)\n",
        "tgt = tgt.to(device)\n",
        "\n",
        "logits = model(inp)   # (B, T, vocab_size)\n",
        "loss = nn.CrossEntropyLoss()(logits.view(-1, vocab_size), tgt.view(-1))\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH4tk1oUGx9E",
        "outputId": "71fb1dd2-9d09-4866-e619-b108a7a11b7c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.6714, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"epochs\": 1,\n",
        "    \"batch_size\": 32,\n",
        "    \"block_size\": 128,\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"warmup_steps\": 200,\n",
        "    \"max_steps\": 10000,  # safety stop\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"log_every\": 100,\n",
        "    \"val_every\": 1000,\n",
        "}\n",
        "\n",
        "wandb.init(project=\"tiny-gpt-training\", config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "sGc86WugehcU",
        "outputId": "444ec56e-d7a4-48bb-decd-683a3f57e135"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▂▄▅▅██████████████████████████████████▇</td></tr><tr><td>step</td><td>▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▆▆▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.00028</td></tr><tr><td>step</td><td>2000</td></tr><tr><td>train/loss</td><td>3.1292</td></tr><tr><td>val/loss</td><td>3.48248</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">toasty-monkey-6</strong> at: <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/nlybxr62' target=\"_blank\">https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/nlybxr62</a><br> View project at: <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training' target=\"_blank\">https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251204_131128-nlybxr62/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251204_131552-dmzs5u88</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/dmzs5u88' target=\"_blank\">robust-universe-7</a></strong> to <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training' target=\"_blank\">https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/dmzs5u88' target=\"_blank\">https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/dmzs5u88</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/dmzs5u88?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e0c184c1df0>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config[\"learning_rate\"],\n",
        "    weight_decay=config[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "# Cosine scheduler with warmup\n",
        "def get_lr(step):\n",
        "    if step < config[\"warmup_steps\"]:\n",
        "        return step / config[\"warmup_steps\"]\n",
        "    progress = (step - config[\"warmup_steps\"]) / max(1, config[\"max_steps\"] - config[\"warmup_steps\"])\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n"
      ],
      "metadata": {
        "id": "ti2PWYL8VvnT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Device + AMP scaler\n",
        "# ------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# ------------------------------\n",
        "# Tracking\n",
        "# ------------------------------\n",
        "best_val_loss = float(\"inf\")\n",
        "steps_since_improve = 0\n",
        "patience = 1000            # stop if no improvement this long\n",
        "global_step = 0\n",
        "\n",
        "# ------------------------------\n",
        "# TRAINING LOOP\n",
        "# ------------------------------\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "    for inp, tgt in pbar:\n",
        "        model.train()\n",
        "        inp, tgt = inp.to(device), tgt.to(device)\n",
        "\n",
        "        # ------------------------------\n",
        "        # Learning rate scheduling\n",
        "        # ------------------------------\n",
        "        lr = config[\"learning_rate\"] * get_lr(global_step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # ------------------------------\n",
        "        # Forward pass (mixed precision)\n",
        "        # ------------------------------\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "            logits = model(inp)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                tgt.view(-1)\n",
        "            )\n",
        "\n",
        "        # Backward\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # ------------------------------\n",
        "        # Logging to W&B\n",
        "        # ------------------------------\n",
        "        wandb.log({\n",
        "            \"train/loss\": loss.item(),\n",
        "            \"lr\": lr,\n",
        "            \"step\": global_step\n",
        "        })\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        # ------------------------------\n",
        "        # PRINT EVERY log_every STEPS\n",
        "        # ------------------------------\n",
        "        if global_step % config[\"log_every\"] == 0:\n",
        "            print(f\"\\n[TRAIN] Step {global_step} | Loss: {loss.item():.4f} | LR: {lr:.6f}\")\n",
        "\n",
        "        # ------------------------------\n",
        "        # VALIDATION\n",
        "        # ------------------------------\n",
        "        if global_step % config[\"val_every\"] == 0 and global_step > 0:\n",
        "\n",
        "            print(f\"\\n[VAL] Running validation at step {global_step}...\")\n",
        "            model.eval()\n",
        "            val_losses = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for v_inp, v_tgt in val_loader:\n",
        "                    v_inp, v_tgt = v_inp.to(device), v_tgt.to(device)\n",
        "                    with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "                        v_logits = model(v_inp)\n",
        "                        v_loss = F.cross_entropy(\n",
        "                            v_logits.view(-1, v_logits.size(-1)),\n",
        "                            v_tgt.view(-1)\n",
        "                        )\n",
        "                    val_losses.append(v_loss.item())\n",
        "\n",
        "            mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "\n",
        "            print(f\"[VAL] Step {global_step} | Val Loss: {mean_val_loss:.4f}\")\n",
        "\n",
        "            wandb.log({\"val/loss\": mean_val_loss, \"step\": global_step})\n",
        "\n",
        "            # ------------------------------\n",
        "            # SMART CHECKPOINTING\n",
        "            # ------------------------------\n",
        "            if mean_val_loss < best_val_loss:\n",
        "                best_val_loss = mean_val_loss\n",
        "                steps_since_improve = 0\n",
        "\n",
        "                ckpt_path = f\"best_model_step_{global_step}.pt\"\n",
        "                torch.save(model.state_dict(), ckpt_path)\n",
        "                wandb.save(ckpt_path)\n",
        "\n",
        "                print(f\"✔ New Best Model Saved! Val loss improved to {best_val_loss:.4f}\")\n",
        "            else:\n",
        "                steps_since_improve += config[\"val_every\"]\n",
        "                print(f\"✘ No improvement for {steps_since_improve} steps\")\n",
        "\n",
        "            # Rolling checkpoint\n",
        "            torch.save(model.state_dict(), \"latest_model.pt\")\n",
        "\n",
        "            # ------------------------------\n",
        "            # EARLY STOPPING\n",
        "            # ------------------------------\n",
        "            if steps_since_improve >= patience:\n",
        "                print(\"\\n############################\")\n",
        "                print(\"### EARLY STOPPING FIRED ###\")\n",
        "                print(\"### No improvement observed ###\")\n",
        "                print(\"############################\\n\")\n",
        "\n",
        "                torch.save(model.state_dict(), \"final_model.pt\")\n",
        "                wandb.finish()\n",
        "                raise SystemExit(\"EARLY STOPPING TRIGGERED.\")\n",
        "\n",
        "        # ------------------------------\n",
        "        # UPDATE STEP + STOP IF MAX REACHED\n",
        "        # ------------------------------\n",
        "        global_step += 1\n",
        "        if global_step >= config[\"max_steps\"]:\n",
        "            break\n",
        "\n",
        "    if global_step >= config[\"max_steps\"]:\n",
        "        break\n",
        "\n",
        "# ------------------------------\n",
        "# FINAL SAVE\n",
        "# ------------------------------\n",
        "torch.save(model.state_dict(), \"final_model.pt\")\n",
        "print(\"\\n🎉 Training Complete!\")\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QoTLoPh3V68T",
        "outputId": "ba4e22ce-2abc-4b8f-de04-f593cb47be78"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3915845230.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Epoch 1/1:   0%|          | 0/66242 [00:00<?, ?it/s]/tmp/ipython-input-3915845230.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "Epoch 1/1:   0%|          | 3/66242 [00:00<2:13:33,  8.27it/s, loss=7.6095]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 0 | Loss: 7.6084 | LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   0%|          | 104/66242 [00:06<56:36, 19.47it/s, loss=6.4328]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 100 | Loss: 6.4987 | LR: 0.000150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   0%|          | 203/66242 [00:11<49:54, 22.05it/s, loss=5.4134]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 200 | Loss: 5.3771 | LR: 0.000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   0%|          | 303/66242 [00:16<1:07:51, 16.20it/s, loss=4.6844]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 300 | Loss: 4.6462 | LR: 0.000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|          | 405/66242 [00:21<49:59, 21.95it/s, loss=4.2286]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 400 | Loss: 4.3363 | LR: 0.000300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|          | 504/66242 [00:26<51:44, 21.18it/s, loss=4.0772]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 500 | Loss: 4.1990 | LR: 0.000299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|          | 604/66242 [00:31<1:08:04, 16.07it/s, loss=3.9619]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 600 | Loss: 3.8487 | LR: 0.000299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|          | 703/66242 [00:37<51:57, 21.02it/s, loss=3.7039]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 700 | Loss: 3.8600 | LR: 0.000298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|          | 805/66242 [00:41<48:48, 22.35it/s, loss=3.7573]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 800 | Loss: 3.8262 | LR: 0.000297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   1%|▏         | 902/66242 [00:47<1:14:33, 14.60it/s, loss=3.6614]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 900 | Loss: 3.6479 | LR: 0.000296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 999/66242 [00:52<51:02, 21.30it/s, loss=3.3487]/tmp/ipython-input-3915845230.py:84: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1000 | Loss: 3.3487 | LR: 0.000295\n",
            "\n",
            "[VAL] Running validation at step 1000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1001/66242 [01:13<42:32:04,  2.35s/it, loss=3.5375]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 1000 | Val Loss: 3.4878\n",
            "✔ New Best Model Saved! Val loss improved to 3.4878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1104/66242 [01:19<53:59, 20.11it/s, loss=3.4845]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1100 | Loss: 3.3838 | LR: 0.000294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1203/66242 [01:24<50:55, 21.28it/s, loss=3.3855]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1200 | Loss: 3.3155 | LR: 0.000292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1302/66242 [01:28<50:02, 21.63it/s, loss=3.3642]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1300 | Loss: 3.2858 | LR: 0.000291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1405/66242 [01:35<50:23, 21.44it/s, loss=3.1658]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1400 | Loss: 3.2560 | LR: 0.000289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1504/66242 [01:39<50:02, 21.56it/s, loss=3.2093]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1500 | Loss: 3.2150 | LR: 0.000287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   2%|▏         | 1603/66242 [01:44<1:04:19, 16.75it/s, loss=3.2269]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1600 | Loss: 3.1539 | LR: 0.000285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 1704/66242 [01:50<49:24, 21.77it/s, loss=3.1813]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1700 | Loss: 3.0738 | LR: 0.000283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 1803/66242 [01:55<54:23, 19.75it/s, loss=3.1425]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1800 | Loss: 3.3045 | LR: 0.000281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 1903/66242 [02:00<1:05:41, 16.32it/s, loss=3.0450]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 1900 | Loss: 3.0550 | LR: 0.000278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 1999/66242 [02:05<49:12, 21.76it/s, loss=3.1225]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2000 | Loss: 3.1225 | LR: 0.000276\n",
            "\n",
            "[VAL] Running validation at step 2000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 2001/66242 [02:26<41:49:55,  2.34s/it, loss=2.9564]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 2000 | Val Loss: 3.0214\n",
            "✔ New Best Model Saved! Val loss improved to 3.0214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 2102/66242 [02:32<1:13:37, 14.52it/s, loss=3.0815]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2100 | Loss: 3.0272 | LR: 0.000273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 2203/66242 [02:37<49:46, 21.44it/s, loss=3.0116]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2200 | Loss: 2.9338 | LR: 0.000270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   3%|▎         | 2303/66242 [02:42<50:15, 21.20it/s, loss=2.9075]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2300 | Loss: 3.0908 | LR: 0.000267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   4%|▎         | 2403/66242 [02:48<1:16:08, 13.97it/s, loss=2.9526]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2400 | Loss: 3.0045 | LR: 0.000264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   4%|▍         | 2503/66242 [02:52<48:47, 21.77it/s, loss=2.9542]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2500 | Loss: 2.9315 | LR: 0.000261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   4%|▍         | 2605/66242 [02:57<49:34, 21.40it/s, loss=3.0590]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2600 | Loss: 2.9884 | LR: 0.000258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   4%|▍         | 2704/66242 [03:03<56:18, 18.81it/s, loss=2.8960]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2700 | Loss: 2.7539 | LR: 0.000254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   4%|▍         | 2803/66242 [03:08<49:45, 21.25it/s, loss=2.8717]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2800 | Loss: 2.7260 | LR: 0.000251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   4%|▍         | 2905/66242 [03:12<46:48, 22.55it/s, loss=2.7526]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 2900 | Loss: 2.7950 | LR: 0.000247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▍         | 3000/66242 [03:18<50:58, 20.68it/s, loss=2.8931]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3000 | Loss: 2.8931 | LR: 0.000244\n",
            "\n",
            "[VAL] Running validation at step 3000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▍         | 3001/66242 [03:39<48:04:18,  2.74s/it, loss=2.8161]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 3000 | Val Loss: 2.8210\n",
            "✔ New Best Model Saved! Val loss improved to 2.8210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▍         | 3103/66242 [03:44<1:05:00, 16.19it/s, loss=2.8616]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3100 | Loss: 2.8174 | LR: 0.000240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▍         | 3204/66242 [03:50<47:45, 22.00it/s, loss=2.8060]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3200 | Loss: 2.7718 | LR: 0.000236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▍         | 3303/66242 [03:54<49:39, 21.12it/s, loss=2.7246]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3300 | Loss: 2.8598 | LR: 0.000232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▌         | 3403/66242 [03:59<1:08:46, 15.23it/s, loss=2.6749]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3400 | Loss: 2.7459 | LR: 0.000228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▌         | 3505/66242 [04:05<48:36, 21.51it/s, loss=2.7189]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3500 | Loss: 2.7964 | LR: 0.000224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   5%|▌         | 3604/66242 [04:10<47:11, 22.12it/s, loss=2.7631]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3600 | Loss: 2.6458 | LR: 0.000219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▌         | 3702/66242 [04:15<1:11:24, 14.60it/s, loss=2.7687]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3700 | Loss: 2.7866 | LR: 0.000215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▌         | 3803/66242 [04:20<47:44, 21.80it/s, loss=2.6329]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3800 | Loss: 2.7299 | LR: 0.000211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▌         | 3902/66242 [04:25<48:27, 21.44it/s, loss=2.5964]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 3900 | Loss: 2.6314 | LR: 0.000206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▌         | 3999/66242 [04:30<1:16:35, 13.54it/s, loss=2.7093]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4000 | Loss: 2.7093 | LR: 0.000202\n",
            "\n",
            "[VAL] Running validation at step 4000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▌         | 4001/66242 [04:51<55:10:56,  3.19s/it, loss=2.7056]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 4000 | Val Loss: 2.7064\n",
            "✔ New Best Model Saved! Val loss improved to 2.7064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▌         | 4103/66242 [04:56<47:10, 21.96it/s, loss=2.6357]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4100 | Loss: 2.7126 | LR: 0.000197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▋         | 4203/66242 [05:02<48:33, 21.29it/s, loss=2.6611]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4200 | Loss: 2.6523 | LR: 0.000193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   6%|▋         | 4305/66242 [05:07<46:26, 22.23it/s, loss=2.7197]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4300 | Loss: 2.6677 | LR: 0.000188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 4404/66242 [05:11<1:00:33, 17.02it/s, loss=2.6956]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4400 | Loss: 2.6842 | LR: 0.000183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 4503/66242 [05:17<49:54, 20.62it/s, loss=2.6400]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4500 | Loss: 2.8097 | LR: 0.000179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 4605/66242 [05:22<48:00, 21.40it/s, loss=2.5035]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4600 | Loss: 2.7069 | LR: 0.000174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 4703/66242 [05:27<1:05:22, 15.69it/s, loss=2.6032]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4700 | Loss: 2.7062 | LR: 0.000169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 4805/66242 [05:33<47:02, 21.76it/s, loss=2.6707]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4800 | Loss: 2.7291 | LR: 0.000164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   7%|▋         | 4904/66242 [05:37<46:45, 21.86it/s, loss=2.6499]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 4900 | Loss: 2.6425 | LR: 0.000160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5000/66242 [05:43<1:04:50, 15.74it/s, loss=2.6395]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5000 | Loss: 2.6395 | LR: 0.000155\n",
            "\n",
            "[VAL] Running validation at step 5000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5001/66242 [06:04<64:57:46,  3.82s/it, loss=2.7036]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 5000 | Val Loss: 2.6271\n",
            "✔ New Best Model Saved! Val loss improved to 2.6271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5104/66242 [06:09<45:29, 22.40it/s, loss=2.6014]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5100 | Loss: 2.6303 | LR: 0.000150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5204/66242 [06:15<1:00:17, 16.87it/s, loss=2.5560]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5200 | Loss: 2.5755 | LR: 0.000145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5305/66242 [06:19<48:01, 21.15it/s, loss=2.5373]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5300 | Loss: 2.6051 | LR: 0.000140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5404/66242 [06:24<46:42, 21.71it/s, loss=2.7008]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5400 | Loss: 2.6534 | LR: 0.000136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5505/66242 [06:30<49:22, 20.50it/s, loss=2.6074]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5500 | Loss: 2.5631 | LR: 0.000131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   8%|▊         | 5604/66242 [06:35<45:21, 22.28it/s, loss=2.7346]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5600 | Loss: 2.5908 | LR: 0.000126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▊         | 5703/66242 [06:39<48:45, 20.69it/s, loss=2.4686]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5700 | Loss: 2.5962 | LR: 0.000121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▉         | 5804/66242 [06:46<46:21, 21.73it/s, loss=2.6472]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5800 | Loss: 2.5355 | LR: 0.000117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▉         | 5903/66242 [06:50<46:58, 21.41it/s, loss=2.6424]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 5900 | Loss: 2.5560 | LR: 0.000112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▉         | 5999/66242 [06:55<59:18, 16.93it/s, loss=2.5788]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6000 | Loss: 2.5788 | LR: 0.000107\n",
            "\n",
            "[VAL] Running validation at step 6000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▉         | 6001/66242 [07:16<49:21:34,  2.95s/it, loss=2.7368]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 6000 | Val Loss: 2.5693\n",
            "✔ New Best Model Saved! Val loss improved to 2.5693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▉         | 6105/66242 [07:21<46:27, 21.58it/s, loss=2.5199]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6100 | Loss: 2.5216 | LR: 0.000103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:   9%|▉         | 6203/66242 [07:27<1:09:56, 14.31it/s, loss=2.6698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6200 | Loss: 2.6154 | LR: 0.000098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|▉         | 6305/66242 [07:32<46:31, 21.47it/s, loss=2.5246]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6300 | Loss: 2.6677 | LR: 0.000094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|▉         | 6404/66242 [07:37<45:49, 21.76it/s, loss=2.5925]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6400 | Loss: 2.5255 | LR: 0.000089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|▉         | 6503/66242 [07:43<1:11:19, 13.96it/s, loss=2.5033]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6500 | Loss: 2.6460 | LR: 0.000085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|▉         | 6604/66242 [07:48<46:24, 21.42it/s, loss=2.5875]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6600 | Loss: 2.6110 | LR: 0.000081\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|█         | 6703/66242 [07:52<45:30, 21.80it/s, loss=2.5524]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6700 | Loss: 2.6852 | LR: 0.000076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|█         | 6805/66242 [07:58<1:00:03, 16.49it/s, loss=2.5153]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6800 | Loss: 2.5382 | LR: 0.000072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|█         | 6904/66242 [08:03<46:13, 21.39it/s, loss=2.6546]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 6900 | Loss: 2.6066 | LR: 0.000068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█         | 7000/66242 [08:07<46:57, 21.02it/s, loss=2.5531]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7000 | Loss: 2.5531 | LR: 0.000064\n",
            "\n",
            "[VAL] Running validation at step 7000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█         | 7001/66242 [08:29<45:32:13,  2.77s/it, loss=2.4677]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 7000 | Val Loss: 2.5291\n",
            "✔ New Best Model Saved! Val loss improved to 2.5291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█         | 7103/66242 [08:34<45:46, 21.53it/s, loss=2.5895]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7100 | Loss: 2.6354 | LR: 0.000060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█         | 7202/66242 [08:39<1:04:55, 15.15it/s, loss=2.4906]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7200 | Loss: 2.5659 | LR: 0.000056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█         | 7305/66242 [08:45<45:35, 21.55it/s, loss=2.4452]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7300 | Loss: 2.5752 | LR: 0.000053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█         | 7404/66242 [08:49<46:57, 20.88it/s, loss=2.4135]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7400 | Loss: 2.5212 | LR: 0.000049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█▏        | 7503/66242 [08:55<1:00:57, 16.06it/s, loss=2.5144]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7500 | Loss: 2.7107 | LR: 0.000046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  11%|█▏        | 7605/66242 [09:00<45:21, 21.54it/s, loss=2.5496]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7600 | Loss: 2.6204 | LR: 0.000042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 7704/66242 [09:05<45:45, 21.32it/s, loss=2.5772]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7700 | Loss: 2.5775 | LR: 0.000039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 7803/66242 [09:10<1:06:57, 14.55it/s, loss=2.6621]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7800 | Loss: 2.4164 | LR: 0.000036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 7903/66242 [09:16<44:39, 21.77it/s, loss=2.5606]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 7900 | Loss: 2.5152 | LR: 0.000033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 7999/66242 [09:20<46:48, 20.74it/s, loss=2.4633]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8000 | Loss: 2.4633 | LR: 0.000030\n",
            "\n",
            "[VAL] Running validation at step 8000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 8001/66242 [09:42<40:00:13,  2.47s/it, loss=2.4608]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 8000 | Val Loss: 2.5035\n",
            "✔ New Best Model Saved! Val loss improved to 2.5035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 8103/66242 [09:47<45:38, 21.23it/s, loss=2.5603]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8100 | Loss: 2.5545 | LR: 0.000027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  12%|█▏        | 8202/66242 [09:51<50:53, 19.01it/s, loss=2.4514]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8200 | Loss: 2.5716 | LR: 0.000024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8304/66242 [09:58<45:25, 21.25it/s, loss=2.5595]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8300 | Loss: 2.4044 | LR: 0.000022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8403/66242 [10:02<45:19, 21.27it/s, loss=2.5738]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8400 | Loss: 2.4463 | LR: 0.000019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8502/66242 [10:07<59:36, 16.15it/s, loss=2.5141]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8500 | Loss: 2.5133 | LR: 0.000017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8605/66242 [10:13<44:00, 21.83it/s, loss=2.4362]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8600 | Loss: 2.5449 | LR: 0.000015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8704/66242 [10:18<45:11, 21.22it/s, loss=2.3813]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8700 | Loss: 2.4375 | LR: 0.000013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8804/66242 [10:23<58:57, 16.24it/s, loss=2.5229]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8800 | Loss: 2.4703 | LR: 0.000011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  13%|█▎        | 8904/66242 [10:29<44:49, 21.32it/s, loss=2.4999]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 8900 | Loss: 2.4938 | LR: 0.000009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▎        | 9000/66242 [10:33<42:34, 22.41it/s, loss=2.5038]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9000 | Loss: 2.5038 | LR: 0.000008\n",
            "\n",
            "[VAL] Running validation at step 9000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▎        | 9001/66242 [10:55<43:32:34,  2.74s/it, loss=2.5038]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAL] Step 9000 | Val Loss: 2.4911\n",
            "✔ New Best Model Saved! Val loss improved to 2.4911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▎        | 9104/66242 [11:00<46:26, 20.51it/s, loss=2.4341]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9100 | Loss: 2.5406 | LR: 0.000006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▍        | 9203/66242 [11:04<46:01, 20.66it/s, loss=2.4911]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9200 | Loss: 2.3538 | LR: 0.000005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▍        | 9304/66242 [11:11<50:27, 18.81it/s, loss=2.4700]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9300 | Loss: 2.4117 | LR: 0.000004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▍        | 9403/66242 [11:15<43:20, 21.86it/s, loss=2.4274]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9400 | Loss: 2.5149 | LR: 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▍        | 9505/66242 [11:20<44:22, 21.31it/s, loss=2.3962]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9500 | Loss: 2.5160 | LR: 0.000002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  14%|█▍        | 9605/66242 [11:26<44:17, 21.31it/s, loss=2.5366]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9600 | Loss: 2.4462 | LR: 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  15%|█▍        | 9704/66242 [11:31<45:26, 20.74it/s, loss=2.4017]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9700 | Loss: 2.6009 | LR: 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  15%|█▍        | 9803/66242 [11:35<50:25, 18.65it/s, loss=2.4127]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9800 | Loss: 2.6875 | LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  15%|█▍        | 9904/66242 [11:41<43:39, 21.50it/s, loss=2.5456]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] Step 9900 | Loss: 2.4251 | LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  15%|█▌        | 9999/66242 [11:46<1:06:13, 14.15it/s, loss=2.5031]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎉 Training Complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>███████▇▇▇▆▆▆▆▆▆▆▆▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/loss</td><td>██▆▆▅▅▄▄▄▃▃▂▂▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>val/loss</td><td>█▅▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>step</td><td>9999</td></tr><tr><td>train/loss</td><td>2.5031</td></tr><tr><td>val/loss</td><td>2.49107</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-universe-7</strong> at: <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/dmzs5u88' target=\"_blank\">https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training/runs/dmzs5u88</a><br> View project at: <a href='https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training' target=\"_blank\">https://wandb.ai/guhan-karthik25-kumaraguru-college-of-technology/tiny-gpt-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 9 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251204_131552-dmzs5u88/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}